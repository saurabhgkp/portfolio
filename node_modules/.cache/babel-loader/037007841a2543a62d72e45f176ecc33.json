{"ast":null,"code":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.tokenMatcher = exports.createTokenInstance = exports.EOF = exports.createToken = exports.hasTokenLabel = exports.tokenName = exports.tokenLabel = void 0;\nvar utils_1 = require(\"@chevrotain/utils\");\nvar lexer_public_1 = require(\"./lexer_public\");\nvar tokens_1 = require(\"./tokens\");\nfunction tokenLabel(tokType) {\n  if (hasTokenLabel(tokType)) {\n    return tokType.LABEL;\n  } else {\n    return tokType.name;\n  }\n}\nexports.tokenLabel = tokenLabel;\nfunction tokenName(tokType) {\n  return tokType.name;\n}\nexports.tokenName = tokenName;\nfunction hasTokenLabel(obj) {\n  return utils_1.isString(obj.LABEL) && obj.LABEL !== \"\";\n}\nexports.hasTokenLabel = hasTokenLabel;\nvar PARENT = \"parent\";\nvar CATEGORIES = \"categories\";\nvar LABEL = \"label\";\nvar GROUP = \"group\";\nvar PUSH_MODE = \"push_mode\";\nvar POP_MODE = \"pop_mode\";\nvar LONGER_ALT = \"longer_alt\";\nvar LINE_BREAKS = \"line_breaks\";\nvar START_CHARS_HINT = \"start_chars_hint\";\nfunction createToken(config) {\n  return createTokenInternal(config);\n}\nexports.createToken = createToken;\nfunction createTokenInternal(config) {\n  var pattern = config.pattern;\n  var tokenType = {};\n  tokenType.name = config.name;\n  if (!utils_1.isUndefined(pattern)) {\n    tokenType.PATTERN = pattern;\n  }\n  if (utils_1.has(config, PARENT)) {\n    throw \"The parent property is no longer supported.\\n\" + \"See: https://github.com/chevrotain/chevrotain/issues/564#issuecomment-349062346 for details.\";\n  }\n  if (utils_1.has(config, CATEGORIES)) {\n    // casting to ANY as this will be fixed inside `augmentTokenTypes``\n    tokenType.CATEGORIES = config[CATEGORIES];\n  }\n  tokens_1.augmentTokenTypes([tokenType]);\n  if (utils_1.has(config, LABEL)) {\n    tokenType.LABEL = config[LABEL];\n  }\n  if (utils_1.has(config, GROUP)) {\n    tokenType.GROUP = config[GROUP];\n  }\n  if (utils_1.has(config, POP_MODE)) {\n    tokenType.POP_MODE = config[POP_MODE];\n  }\n  if (utils_1.has(config, PUSH_MODE)) {\n    tokenType.PUSH_MODE = config[PUSH_MODE];\n  }\n  if (utils_1.has(config, LONGER_ALT)) {\n    tokenType.LONGER_ALT = config[LONGER_ALT];\n  }\n  if (utils_1.has(config, LINE_BREAKS)) {\n    tokenType.LINE_BREAKS = config[LINE_BREAKS];\n  }\n  if (utils_1.has(config, START_CHARS_HINT)) {\n    tokenType.START_CHARS_HINT = config[START_CHARS_HINT];\n  }\n  return tokenType;\n}\nexports.EOF = createToken({\n  name: \"EOF\",\n  pattern: lexer_public_1.Lexer.NA\n});\ntokens_1.augmentTokenTypes([exports.EOF]);\nfunction createTokenInstance(tokType, image, startOffset, endOffset, startLine, endLine, startColumn, endColumn) {\n  return {\n    image: image,\n    startOffset: startOffset,\n    endOffset: endOffset,\n    startLine: startLine,\n    endLine: endLine,\n    startColumn: startColumn,\n    endColumn: endColumn,\n    tokenTypeIdx: tokType.tokenTypeIdx,\n    tokenType: tokType\n  };\n}\nexports.createTokenInstance = createTokenInstance;\nfunction tokenMatcher(token, tokType) {\n  return tokens_1.tokenStructuredMatcher(token, tokType);\n}\nexports.tokenMatcher = tokenMatcher;","map":null,"metadata":{},"sourceType":"script"}